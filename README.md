# ğŸ‘£ AI-Gait-Visualizer

[![Launch on Hugging Face](https://img.shields.io/badge/Launch-ğŸŸ§%20Hugging%20Face%20Demo-orange)](https://huggingface.co/spaces/rmehmood786/AI-Gait-Visualizer)

---

### ğŸ“ Overview
**AI-Gait-Visualizer** is an interactive application that extracts human pose keypoints and gait metrics from short walking videos.  
It uses **YOLOv8-Pose** for keypoint detection and a lightweight gait-analysis pipeline to estimate stride length proxies and cadence.
---

### ğŸš€ Live Demo
ğŸŸ¢ **Try it directly here:**  
ğŸ‘‰ [https://huggingface.co/spaces/rmehmood786/AI-Gait-Visualizer](https://huggingface.co/spaces/rmehmood786/AI-Gait-Visualizer)

Upload a walking video â†’ click **Run Analysis** â†’ view the annotated video and download CSV metrics.

---

### âš™ï¸ Features
- ğŸ¯ **Pose Estimation:** Real-time body keypoints using YOLOv8-Pose.  
- ğŸ¦¶ **Gait Metrics:** Calculates stride proxy, step duration, and approximate cadence.  
- ğŸ“Š **Downloadable Results:** Exports a CSV file with per-frame gait data.  
- ğŸ’¡ **Interactive Interface:** Built with Gradio for smooth use.  
- â˜ï¸ **Cloud-Ready:** Runs directly on Hugging Face Spaces.

---

### ğŸ§© Tech Stack
| Component | Description |
|------------|-------------|
| **Model** | YOLOv8-Pose (Ultralytics) |
| **Frameworks** | Gradio, OpenCV, NumPy, SciPy, Pandas |
| **Deployment** | Hugging Face Spaces |
| **Language** | Python 3.10+ |

---

### ğŸ—‚ï¸ Project Structure
```
AI-Gait-Visualizer/
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ app.py              # Gradio app entry point
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ gait_viz.py         # Gait extraction + pose processing
â”‚
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ app_file                 # tells Hugging Face to run app/app.py
â””â”€â”€ README.md
```

---

### ğŸ§  How It Works
1. The uploaded video is read frame-by-frame using OpenCV.  
2. YOLOv8-Pose detects body keypoints per frame.  
3. Gait parameters (stride proxy, step duration, cadence) are computed from keypoint motion.  
4. Processed frames are written back into an annotated video.  
5. The app displays results and offers a downloadable CSV.

---

### ğŸ–¥ï¸ Run Locally
```bash
git clone https://github.com/rmehmood786/AI-Gait-Visualizer.git
cd AI-Gait-Visualizer
pip install -r requirements.txt
python app/app.py
```
Then open the local Gradio link in your browser.

---

### ğŸ“¦ Requirements
```
gradio>=4.0.0
ultralytics>=8.2.0
opencv-python>=4.10.0
numpy>=1.24
pandas>=2.1
scipy>=1.11
matplotlib>=3.8
tqdm
rich
```

---

### ğŸ¬ Example Output
| Input | Output |
|-------|---------|
| <img src="https://raw.githubusercontent.com/rmehmood786/AI-Gait-Visualizer/main/Sample_input.gif" width="300"/> | <img src= "sample_output.gif" width="300"/> |
---
## ğŸ’¡ Ideas to Extend or Experiment With

- Integrate pose estimation with gait recognition for identity prediction.  
- Visualize motion trajectories in 3D using Open3D or Matplotlib.  
- Add real-time gait visualization using webcam input.  
- Try comparing different YOLO variants (v8, v10) for accuracy vs. speed.  
- Use temporal smoothing or Kalman filtering for stable keypoints.  
- Build a web-based dashboard for visualizing stride metrics interactively.
---

### ğŸ‘¤ Author
**Rashid Mehmood**  
AI Researcher â€” Computer Vision & Gait Analysis  
- ğŸŒ [GitHub](https://github.com/rmehmood786)  
- ğŸ¤— [Hugging Face Space](https://huggingface.co/spaces/rmehmood786/AI-Gait-Visualizer)

---

### ğŸªª License
Released under the **MIT License**.  
You are free to use, modify, and share with proper attribution.

---

### â­ Acknowledgements
Built using:
- [Ultralytics YOLOv8](https://github.com/ultralytics/ultralytics)  
- [Gradio](https://github.com/gradio-app/gradio)